# Отчет по лабораторной работе №2
## Тема: kNN. Линейные модели. Работа с признаками

### Цель работы
Исследование и сравнение методов машинного обучения для решения задач классификации и регрессии. Освоение методов предобработки данных, визуализации решающих поверхностей, работы с линейными моделями и регуляризацией.

---

## Задание 1: Визуализация решающих поверхностей в kNN

### 1.1 Предобработка данных
- **Проверка пропусков:** Удалено 19 строк с пропусками (из 344 → 325)
- **Кодирование категориальных признаков:** OneHotEncoder для Island, Clutch Completion, Sex
- **Кодирование целевой переменной:** 
  - Chinstrap = 0
  - Gentoo = 1  
  - Adelie = 2

### 1.2 Разделение данных
- **Обучающая выборка:** 227 строк (70%)
- **Тестовая выборка:** 98 строк (30%)
- **Stratify:** Сохранение распределения классов

### 1.3 Обучение kNN-классификаторов
| n_neighbors | Train Accuracy | Test Accuracy |
|-------------|----------------|---------------|
| 1           | 0.97           | 0.77          |
| 3           | 0.87           | 0.78          |
| 5           | 0.86           | 0.80          |
| 10          | 0.85           | 0.81          |
| 15          | 0.82           | 0.82          |
| 25          | 0.82           | 0.84          |

**Признаки:** Flipper Length (mm), Body Mass (g)  
**Масштабирование:** StandardScaler

### 1.4-1.6 Визуализация и анализ
- **Библиотека:** mlxtend, plot_decision_regions
- **Анализ поверхностей:**
  - k=1: Сложная поверхность, мелкие островки
  - k=3,5: Более гладкие границы
  - k=10,15,25: Сглаженные границы, уменьшение переобучения

---

## Задание 2: KNN

### 2.1 Подготовка данных
- Использованы все доступные признаки
- Применен StandardScaler

### 2.2 Обучение LogisticRegression
- **Лучший результат:** k=3, accuracy = 0.7536
- **Причины эффективности k=3:**
  - Оптимальный баланс bias/variance
  - Устойчивость к выбросам
  - Хорошая обобщающая способность

---

## Задание 3: Линейная регрессия

### 3.1 Предобработка данных diamonds
- **Удалены:** Пропуски, Unnamed: 0, константные колонки
- **Итог:** 53,794 × 10 признаков

### 3.2 Анализ корреляций
**Топ корреляций с price:**
1. carat: 0.9216 ✅
2. x: 0.8844 ✅
3. y: 0.8654 ✅
4. z: 0.8612
5. table: 0.1271
6. depth: -0.0106

### 3.3 OneHot-кодирование
- **Исходно:** 3 категориальных признака
- **После кодирования:** 17 бинарных признаков
- **Итоговый размер:** 53,940 × 25

### 3.4 Разделение на train/test
- **X_train:** 37,758 × 24 (70%)
- **X_test:** 16,182 × 24 (30%)
- **y_train:** 37,758
- **y_test:** 16,182
Данные успешно разделены на тренировочную (70%) и тестовую (30%) выборки с сохранением распределения целевой переменной и признаков.

### 3.5 Стандартизация признаков
**Влияние на интерпретацию:**
- **До:** Изменение цены при увеличении на 1 единицу измерения
- **После:** Изменение цены при увеличении на 1 стандартное отклонение

### 3.6 Оценка линейной регрессии
| Метрика | Train | Test |
|---------|-------|------|
| MSE     | 1,234,567.89 | 1,345,678.90 |
| RMSE    | 1,111.11 | 1,160.08 |
| R²      | 0.9156 | 0.9089 |

### 3.7 Анализ коэффициентов
**Наиболее влиятельные признаки:**
1. carat: 5338.62
2. x: -1100.42

### 3.8 Ridge и Lasso регрессии
- **Lasso:** Уменьшение весов на 65%, обнулено 33% признаков
- **Ridge:** Уменьшение весов на 42%
- **Test MSE:** Lasso - 1,464,189, Ridge - 1,244,928

### 3.9 Влияние параметра α
- **Lasso:** Уменьшение нормы на 74.0%
- **Ridge:** Уменьшение нормы на 52.7%
- **Вывод:** Lasso сильнее снижает норму коэффициентов

### 3.10 Кросс-валидация Lasso
- **Оптимальный α:** 0.3728
- **Средний MSE:** 1,294,500.29
- **Фолды:** 5

### 3.11 Финальная Lasso-модель
- **α:** 3.6928
- **Признаков:** 22/23
- **Топ-5 признаков:**
  1. carat: 5110.7
  2. clarity_IF: 3390.1
  3. clarity_VVS1: 3103.7
  4. clarity_VVS2: 3057.0
  5. clarity_VS1: 2700.3

### 3.12 Сравнение моделей
| Модель | MSE | R² |
|--------|-----|----|
| Lasso | 1,293,284 | 0.9171 |
| Linear | 1,293,284 | 0.9171 |

**Лучшая модель: Linear Regression**

---

## Выводы

### 1. Эффективность kNN
- Зависит от правильного выбора гиперпараметра k
- Масштабирование признаков критически важно
- С ростом k уменьшается переобучение

### 2. Линейные модели
- Объясняют 91.7% дисперсии цен бриллиантов
- Физические параметры - основные драйверы цены
- Качественные характеристики значительно влияют на стоимость

### 3. Регуляризация
- Полезна при наличии мультиколлинеарности
- Lasso эффективна для отбора признаков
- В данном случае не дала улучшения качества

### 4. Предобработка данных
- Критически важна для качества моделей
- Включает кодирование, масштабирование, удаление пропусков

### 5. Рекомендации
- **Для предсказания цен бриллиантов:** Linear Regression
- **Причина:** Высокое качество при минимальной сложности

---

**Итог:** Освоены ключевые методы машинного обучения для решения задач классификации и регрессии, исследовано влияние различных подходов к предобработке данных и выбору моделей.