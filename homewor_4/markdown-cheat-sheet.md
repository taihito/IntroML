# Лабораторная работа 4: Деревья. Случайный лес

## Часть 1: Основы построения решающего дерева

### Задание 1.1: Расчет энтропии

**Дано:**
- 10 объектов в вершине
- 8 объектов класса \( k_1 \)
- 2 объекта класса \( k_2 \)
- Используется натуральный логарифм

**Ответ:** 0.50

### Задание 1.2: Критерий информативности с индексом Джини

**Дано:**
- 12 объектов в вершине
- 9 объектов класса \( k_1 \)
- 3 объекта класса \( k_2 \)

**Ответ:** 0.32

### Задание 1.3: Предсказание в листе для задачи регрессии

**Дано:**
Значения целевой переменной в листе: 
\[
[1, 10, 5, 18, 100, 30, 50, 61, 84, 47]
\]

**Ответ:** 40.6

## Часть 2: Решающие деревья

### Задание 2.1: Функция `find_best_split()`

Тестирование на датасете California Housing.

**Пример данных:**
Первые 5 строк данных:

| MedInc | HouseAge | AveRooms | AveBedrms | Population | AveOccup | Latitude | Longitude |
|--------|----------|----------|-----------|------------|----------|----------|-----------|
| 8.3252 | 41.0     | 6.984127 | 1.023810  | 322.0      | 2.555556 | 37.88    | -122.23   |
| 8.3014 | 21.0     | 6.238137 | 0.971880  | 2401.0     | 2.109842 | 37.86    | -122.22   |
| 7.2574 | 52.0     | 8.288136 | 1.073446  | 496.0      | 2.802260 | 37.85    | -122.24   |
| 5.6431 | 52.0     | 5.817352 | 1.073059  | 558.0      | 2.547945 | 37.85    | -122.25   |
| 3.8462 | 52.0     | 6.281853 | 1.081081  | 565.0      | 2.181467 | 37.85    | -122.25   |

Первые 10 значений целевой переменной:
\[
[4.526, 3.585, 3.521, 3.413, 3.422, 2.697, 2.992, 2.414, 2.267, 2.611]
\]

**Анализ признаков:**
Поиск лучшего признака для первой вершины:

| Признак     | Лучший порог | Выигрыш   |
|-------------|--------------|-----------|
| MedInc      | 5.0351       | 0.412751  |
| HouseAge    | 51.5000      | 0.030901  |
| AveRooms    | 6.3743       | 0.159208  |
| AveBedrms   | 1.1034       | 0.015253  |
| Population  | 1160.5000    | 0.002831  |
| AveOccup    | 3.1125       | 0.073919  |
| Latitude    | 37.9350      | 0.090334  |
| Longitude   | -121.8650    | 0.050211  |


**Вывод:** Лучшим признаком для разбиения является `MedInc` с порогом 5.0351 и максимальным выигрышем 0.412751.

### Задание 2.2: Метод `_predict_node()`

**Характеристики данных:**
- Размерность: (100, 3) - 100 наблюдений с 3 признаками
- Классы: [0, 1] - задача бинарной классификации
- Accuracy: 0.990 - точность 99%

**Важность признаков:**

| Признак   | Важность | Процент | Ранг |
|-----------|----------|---------|------|
| Признак 2 | 0.8832   | 88.3%   | 1    |
| Признак 1 | 0.0763   | 7.6%    | 2    |
| Признак 0 | 0.0405   | 4.1%    | 3    |

**Анализ:** Признак 2 является наиболее информативным для классификации.

### Задание 2.3: Анализ Student Performance

**Основная информация:**
- Количество наблюдений: 258 студентов
- Количество признаков: 7
- Целевая переменная: UNS (0 = низкая успеваемость, 1 = высокая успеваемость)

**Признаки:**
1. Unnamed: 0 - индексная колонка
2. STG - время изучения материала
3. SCG - количество повторений материала
4. STR - время работы с заданиями
5. LPR - успешность выполнения заданий
6. PEG - успех на экзамене
7. UNS - целевая переменная

**Распределение классов:**
- Класс 0 (низкая успеваемость): 107 студентов (41.5%)
- Класс 1 (высокая успеваемость): 151 студент (58.5%)

**Статистически значимые признаки:**

| Признак | Разница | t-статистика | p-value | Интерпретация |
|---------|---------|--------------|---------|---------------|
| PEG     | 0.434   | -26.947      | 0.000   | Экзаменационные результаты наиболее значимы |
| STG     | 0.091   | значимо      | 0.000   | Время изучения важно |
| SCG     | 0.085   | значимо      | 0.001   | Повторение материала важно |
| STR     | 0.088   | значимо      | 0.004   | Время работы с заданиями важно |

**Качество признаков:**

| Признак    | max_gain | Качество  |
|------------|----------|-----------|
| PEG        | 0.399    | ОТЛИЧНЫЙ  |
| STG        | 0.031    | ПЛОХОЙ    |
| LPR        | 0.031    | ПЛОХОЙ    |
| SCG        | 0.020    | ПЛОХОЙ    |
| STR        | 0.018    | ПЛОХОЙ    |
| Unnamed: 0 | 0.009    | ПЛОХОЙ    |

### Задание 2.4: Анализ на датасете mushrooms

**Результаты:**
- Размер датасета: (8124, 23)
- Классы: ['p' (poisonous), 'e' (edible)]
- Обучающая выборка: (4062, 22)
- Тестовая выборка: (4062, 22)
- Количество признаков: 22
- **Accuracy: 1.0000**

**Вывод:** Модель идеально классифицирует грибы на съедобные и ядовитые.

## Часть 3: Бэггинг и случайный лес

### Задание 3.1: Разделение данных Pima Indians Diabetes

**Данные:**
- Размер: (768, 9)
- Признаки: 8 медицинских показателей
- Целевая переменная: Outcome (0 = нет диабета, 1 = диабет)

**Разделение данных:**
1. Первое разделение: 70% train, 30% test
   - X_train: (537, 8) - 69.9%
   - X_test: (231, 8) - 30.1%

2. Второе разделение (из train): 70% train_train, 30% train_val
   - X_train_train: (375, 8) - 48.8%
   - X_train_val: (162, 8) - 21.1%
   - X_test: (231, 8) - 30.1%

**Распределение классов сохранилось пропорционально.**

### Задание 3.2: Подбор гиперпараметров для Decision Tree

**Результаты подбора:**
- Лучшие параметры: `{'max_depth': 5, 'min_samples_leaf': 20}`
- Лучший F1-score: 0.6250

**Тестовая производительность:**
- Accuracy: 0.9668
- Precision: 0.9447
- Recall: 0.9889
- AUC-ROC: 0.9963

### Задание 3.3: BaggingClassifier

**Конфигурация:**
- Количество деревьев: 50
- Обучение на полной тренировочной выборке

**Тестовая производительность:**
- Accuracy: 0.9000
- Precision: 0.7778
- Recall: 1.0000
- AUC-ROC: 1.0000

### Задание 3.4: Random Forest с кросс-валидацией

**Результаты подбора:**
- Лучшие параметры: `{'max_depth': 3, 'min_samples_split': 2}`
- Лучший F1-score: 0.9450

**Тестовая производительность:**
- Accuracy: 0.9000
- Precision: 0.7778
- Recall: 1.0000
- AUC-ROC: 1.0000

**Сравнение моделей:**

| Модель         | Accuracy | Precision | Recall | AUC-ROC |
|----------------|----------|-----------|--------|---------|
| Decision Tree  | 0.7532   | 0.6667    | 0.5185 | 0.7531  |
| Bagging        | 0.7403   | 0.6250    | 0.5185 | 0.7444  |
| Random Forest  | 0.9000   | 0.7778    | 1.0000 | 1.0000  |

**Вывод:** Random Forest показал наилучшие результаты.

### Задание 3.5: Зависимость AUC-ROC от количества деревьев

**Результаты:**

| Деревьев | AUC-ROC |
|----------|---------|
| 1        | 0.7692  |
| 5        | 0.9396  |
| 10       | 0.9945  |
| 20       | 1.0000  |
| 30       | 1.0000  |
| 40       | 0.9890  |
| 50       | 0.9945  |
| 75       | 1.0000  |
| 100      | 0.9890  |
| 150      | 0.9780  |
| 200      | 0.9780  |

**Анализ:**
- Максимальный AUC-ROC: 1.0000 (при 20, 30, 75 деревьях)
- Минимальный: 0.7692 (1 дерево)
- Оптимальный диапазон: 20-75 деревьев

### Задание 3.6: Важность признаков в Random Forest

**Ранжирование признаков:**

1. **Glucose**: 0.2792 
2. **BMI**: 0.1600 
3. **Age**: 0.1319 
4. **DiabetesPedigreeFunction**: 0.1182 
5. **Pregnancies**: 0.1002
6. **BloodPressure**: 0.0815 
7. **Insulin**: 0.0672 
8. **SkinThickness**: 0.0618 

**Вывод:** Самый важный признак: Glucose

## Общие выводы по лабораторной работе

1. **Решающие деревья** эффективны для интерпретируемых моделей
2. **Случайный лес** превосходит отдельные деревья по точности и устойчивости
3. **Важность признаков** помогает понять, какие факторы наиболее значимы
4. **Количество деревьев** в ансамбле важно оптимизировать
5. **Bagging** уменьшает дисперсию и улучшает обобщающую способность

