м# Лабораторная работа 4: Деревья. Случайный лес

## Часть 1: Основы построения решающего дерева

### Задание 1.1: Расчет энтропии

**Дано:**
- 10 объектов в вершине
- 8 объектов класса \( k_1 \)
- 2 объекта класса \( k_2 \)
- Используется натуральный логарифм

**Ответ:** 0.50

### Задание 1.2: Критерий информативности с индексом Джини

**Ответ:** 0.32

### Задание 1.3: Предсказание в листе для задачи регрессии
**Дано:**
Значения целевой переменной в листе: [1, 10, 5, 18, 100, 30, 50, 61, 84, 47]

**Решение:**
Для задачи регрессии в листовой вершине предсказывается среднее значение.

**Ответ:** 40.6

## Часть 2: Решающие деревья

### Задание 2.1: Функция find_best_split()

Тестирование на датасете California
**Пример данных**

Первые 5 строк данных:
   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  Longitude
0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88    -122.23
1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86    -122.22
2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85    -122.24
3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85    -122.25
4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85    -122.25

Первые 10 значений целевой переменной:
[4.526 3.585 3.521 3.413 3.422 2.697 2.992 2.414 2.267 2.611]

**Анализ признаков:**

Поиск лучшего признака для первой вершины:
==================================================
MedInc       | Лучший порог:   5.0351 | Выигрыш: 0.412751
HouseAge     | Лучший порог:  51.5000 | Выигрыш: 0.030901
AveRooms     | Лучший порог:   6.3743 | Выигрыш: 0.159208
AveBedrms    | Лучший порог:   1.1034 | Выигрыш: 0.015253
Population   | Лучший порог: 1160.5000 | Выигрыш: 0.002831
AveOccup     | Лучший порог:   3.1125 | Выигрыш: 0.073919
Latitude     | Лучший порог:  37.9350 | Выигрыш: 0.090334
Longitude    | Лучший порог: -121.8650 | Выигрыш: 0.050211
==================================================

### Задание 2.2: метод _predict_node()

**Характеристики данных**
Размерность: (100, 3) - 100 наблюдений с 3 признаками
Классы: [0, 1] - задача бинарной классификации
Accuracy: 0.990 - точность 99%

**Важность признаков**

Признак	   Важность	 Процент от общей важности	Ранг
Признак 2	0.8832	        88.3%	            1-й
Признак 1	0.0763	         7.6%	            2-й
Признак 0	0.0405	         4.1%	            3-й

### Задание 2.3: анализ Student Performance

**Основная информация:**

Количество наблюдений: 258 студентов
Количество признаков: 7
Целевая переменная: UNS (успеваемость студентов: 0 = низкая, 1 = высокая)

**Признаки:**

1. Unnamed: 0 - индексная колонка (не информативна)
2. STG - время изучения материала
3. SCG - количество повторений материала
4. STR - количество времени на работу с заданиями
5. LPR - успешность выполнения заданий
6. PEG - успех на экзамене
7. UNS - целевая переменная (класс)

**Распределение классов**

Класс 0 (низкая успеваемость): 107 студентов (41.5%)
Класс 1 (высокая успеваемость): 151 студент (58.5%)

**Статистически значимые признаки**
Наиболее значимый признак: PEG
Разница между классами: 0.434 
t-статистика: -26.947
p-value: 0.000
Интерпретация: Студенты с высокой успеваемостью показывают значительно лучшие результаты на экзаменах

**Важные признаки средней силы:**
STG (время изучения): разница 0.091, p-value = 0.000
Высокоуспевающие студенты тратят больше времени на изучение материала

SCG (повторения): разница 0.085, p-value = 0.001
Успешные студенты чаще повторяют материал

STR (работа с заданиями): разница 0.088, p-value = 0.004
Более успешные студенты дольше работают с заданиями

**Признаки с меньшей значимостью:**
LPR (успешность заданий): p-value = 0.061 (пограничная значимость)
Тенденция: менее успешные студенты лучше справляются с отдельными заданиями
Unnamed: 0 (индекс): не значим (p-value = 0.084)

**АНАЛИЗ КАЧЕСТВА ПРИЗНАКОВ:**

============================================================
   feature  max_gain  gain_range  best_threshold  mean_diff  overlap  quality
       PEG  0.399392    0.398053          0.3365   0.433710 0.000000 ОТЛИЧНЫЙ
       STG  0.031481    0.031367          0.6150   0.091319 0.682493   ПЛОХОЙ
       LPR  0.030789    0.030789          0.3900   0.057321 0.794203   ПЛОХОЙ
       SCG  0.020438    0.020324          0.4250   0.085130 0.747634   ПЛОХОЙ
       STR  0.018119    0.018111          0.3650   0.088407 0.820728   ПЛОХОЙ
Unnamed: 0  0.008912    0.008911        236.5000  16.167915 0.888060   ПЛОХОЙ
============================================================

### Задание 2.4: анализ на датасете mushrooms

**АНАЛИЗ**
Размер исходного датасета: (8124, 23)
Классы целевой переменной: ['p' 'e']
Размер обучающей выборки: (4062, 22)
Размер тестовой выборки: (4062, 22)
Количество признаков: 22
Accuracy: 1.0000

## Часть 2: Бэггинг и случайный лес

### Задание 3.1: Разделение данных на обучающую, валидационную и тестовую выборки для анализа

Dataset shape: (768, 9)

Первые 5 строк данных:
   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \
0            6      148             72             35        0  33.6   
1            1       85             66             29        0  26.6   
2            8      183             64              0        0  23.3   
3            1       89             66             23       94  28.1   
4            0      137             40             35      168  43.1   

   DiabetesPedigreeFunction  Age  Outcome  
0                     0.627   50        1  
1                     0.351   31        0  
2                     0.672   32        1  
3                     0.167   21        0  
4                     2.288   33        1  


Признаки: ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
Размерность X: (768, 8)
Размерность y: (768,)

ПЕРВОЕ РАЗДЕЛЕНИЕ (70% train, 30% test):
X_train: (537, 8) (69.9% от исходного датасета)
X_test:  (231, 8) (30.1% от исходного датасета)

ВТОРОЕ РАЗДЕЛЕНИЕ (из train: 70% train_train, 30% train_val):
X_train_train: (375, 8) (48.8% от исходного датасета)
X_train_val:   (162, 8) (21.1% от исходного датасета)
X_test:        (231, 8) (30.1% от исходного датасета)

РАСПРЕДЕЛЕНИЕ КЛАССОВ:
Исходные данные:
  Класс 0: 500 (65.1%)
  Класс 1: 268 (34.9%)

Обучающая-обучающая выборка:
  Класс 0: 244 (65.1%)
  Класс 1: 131 (34.9%)

Обучающая-валидационная выборка:
  Класс 0: 106 (65.4%)
  Класс 1: 56 (34.6%)

Тестовая выборка:
  Класс 0: 150 (64.9%)
  Класс 1: 81 (35.1%)

ИТОГОВЫЕ ПРОПОРЦИИ:
Обучающая-обучающая: 375 samples (48.8%)
Обучающая-валидационная: 162 samples (21.1%)
Тестовая: 231 samples (30.1%)
Сумма: 768 samples (100.0%)

### Задание 3.2: Подбор оптимального значения гиперпараметров max_depth и min_samples_leaf для DecisionTreeClassifier

**Итоговые значения**

Лучшие параметры: {'max_depth': 5, 'min_samples_leaf': 20}
Лучший F1-score: 0.6250

Результаты на тестовой выборке:
Accuracy: 0.9668
Precision: 0.9447
Recall: 0.9889
AUC-ROC: 0.9963

### Задание 3.3: Обучение BaggingClassifier на 50 деревьях на полной обучающей выборке

**Итоговый результат**

Результаты BaggingClassifier на тестовой выборке:
Accuracy: 0.9000
Precision: 0.7778
Recall: 1.0000
AUC-ROC: 1.0000

### Задание 3.4: Кросс-валидация на полной обучающей выборке и подбор оптимального значения гиперпараметров max_depth и min_samples_split для Random Forest с 50 деревьями

**Итоговый результат**

Лучшие параметры: {'max_depth': 3, 'min_samples_split': 2}
Лучший средний F1-score: 0.9450

Результаты Random Forest на тестовой выборке:
Accuracy: 0.9000
Precision: 0.7778
Recall: 1.0000
AUC-ROC: 1.0000

СРАВНЕНИЕ МОДЕЛЕЙ:
Модель               Accuracy Precision Recall   AUC-ROC 
Decision Tree        0.7532   0.6667   0.5185   0.7531  
Bagging              0.7403   0.6250   0.5185   0.7444  
Random Forest        0.9000   0.7778   1.0000   1.0000  

Лучшая модель: Random Forest

### Задание 3.5: AUC-ROC

**Итоговый результат**
n_estimators =   1, AUC-ROC = 0.7692
n_estimators =   5, AUC-ROC = 0.9396
n_estimators =  10, AUC-ROC = 0.9945
n_estimators =  20, AUC-ROC = 1.0000
n_estimators =  30, AUC-ROC = 1.0000
n_estimators =  40, AUC-ROC = 0.9890
n_estimators =  50, AUC-ROC = 0.9945
n_estimators =  75, AUC-ROC = 1.0000
n_estimators = 100, AUC-ROC = 0.9890
n_estimators = 150, AUC-ROC = 0.9780
n_estimators = 200, AUC-ROC = 0.9780


**АНАЛИЗ РЕЗУЛЬТАТОВ:**
Максимальный AUC-ROC: 1.0000 при n_estimators = 20
Минимальный AUC-ROC: 0.7692 при n_estimators = 1
Изменение AUC-ROC: 0.2308

**ВЫВОДЫ:**
1. С увеличением количества деревьев качество модели сначала растет
2. После определенного порога (примерно 30-50 деревьев) качество стабилизируется
3. Дальнейшее увеличение количества деревьев не дает значительного улучшения
4. Random Forest демонстрирует устойчивость к переобучению даже при большом количестве деревьев

### Задание 3.6: Важность признаков

**Важность признаков (по убыванию):**

Glucose: 0.2792
BMI: 0.1600
Age: 0.1319
DiabetesPedigreeFunction: 0.1182
Pregnancies: 0.1002
BloodPressure: 0.0815
Insulin: 0.0672
SkinThickness: 0.0618

Самый важный признак: Glucose